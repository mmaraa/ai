{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Azure OpenAI Own Data - Local CSV\n",
    "\n",
    "This notebook contains an example how to use local CSV-file as own data with Azure Open AI Services. You can use your own CSV in prompt or as a separate data source. With Python (or any other language that supports data handling), you can find the most important lines from CSV and make questions based on that. This is not usually the best way to access external data, but in some cases it might be useful. When you want to access better external data (such as many data from files), use Retrieval Augmented Generation (RAG) to access content. More of those later in other notebooks.\n",
    "\n",
    "At the end, this notebook opened my eyes more, how embeddings are working with AI and why those are so important to find the correct answer from your data.\n",
    "\n",
    "During creation of this notebook I had inspiration from [Shweta Lodha's video](https://www.youtube.com/watch?v=wdhWQuGnnwo). It does not work anymore as it is created with older version of OpenAI services, but I have used her idea of local CSV managing.\n",
    "\n",
    "\n",
    "## Pre-requirements \n",
    "\n",
    "- Create OpenAI service to Azure and deploy at least one model. Fill your own *config.jsonc* file. You can find an example file from *example-config.jsonc*. \n",
    "- CSV file that has some data. You can use mine AI-generated *synchro_elements.csv* or your own file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize OpenAI service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade --quiet openai pandas tiktoken numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import configuration and initliaze client\n",
    "from openai import AzureOpenAI\n",
    "import json\n",
    "\n",
    "config = json.load(open('config.jsonc'))\n",
    "\n",
    "client = AzureOpenAI(\n",
    "    api_version=config['azure_oai_api_version'],\n",
    "    azure_endpoint=config['azure_oai_endpoint'],\n",
    "    api_key=config['azure_oai_key']\n",
    ")\n",
    "gpt_model_name=config['azure_oai_gpt_model_name']\n",
    "embedding_name=config['azure_oai_embedding_model_name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import local CSV\n",
    "\n",
    "We import local CSV file as panda dataframe, summarize it and count tokens that they will spend during the prompt. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tiktoken\n",
    "import numpy as np\n",
    "\n",
    "# Set up tiktoken\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "# Read the CSV data\n",
    "df = pd.read_csv('synchro_elements.csv',delimiter=';')\n",
    "df[\"summarized\"] = (\"abbreviation: \" + df[\"abbreviation\"] + \"; name: \" + df[\"name\"] + \"; description: \" + df[\"description\"] + \"; level_features: \" + df[\"level_features\"])\n",
    "df[\"tokens\"] = df[\"summarized\"].apply(lambda x: len(encoding.encode(str(x))))\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create functions how to handle the data\n",
    "\n",
    "All functions are described before each cell.\n",
    "\n",
    "### Get_text_embedding\n",
    "\n",
    "Get an embedding (vectors) for a single text.\n",
    "\n",
    "### Get_dataframe_embeddings\n",
    "\n",
    "Get embeddings for each text in dataframe and return indexes of dataframe with their vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding functions\n",
    "# Get embeddings for a single text\n",
    "def get_text_embedding(text):\n",
    "    result = client.embeddings.create(\n",
    "        model=embedding_name,\n",
    "        input=text\n",
    "    )\n",
    "    return result.data[0].embedding\n",
    "\n",
    "# Get embeddings for a dataframe\n",
    "def get_dataframe_embeddings(df: pd.DataFrame) -> dict[tuple[str, str], list[float]]:\n",
    "    return { idx: get_text_embedding(r.summarized) for idx, r in df.iterrows() }\n",
    "\n",
    "csv_embeddings = get_dataframe_embeddings(df)\n",
    "csv_embeddings\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate_vector_similarity\n",
    "\n",
    "Get two vectors and calculate similarity of those with numpy's *np.dot()*-function.\n",
    "\n",
    "### Get_docs_with_similarity\n",
    "\n",
    "Get query and embedding dictionary and return similarities of the query's embeddings compared to the dictionary. Return values are sorted from the highest similarity to the lowest similarity.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate similarity by taking in two vectors and returning the dot product (best match)\n",
    "def calculate_vector_similarity(x: list[float], y: list[float]) -> float:\n",
    "    return np.dot(np.array(x), np.array(y))\n",
    "\n",
    "# Get query and dictonary of embeddings and return a list of tuples with the similarity and the index of the document\n",
    "def get_docs_with_similarity(query: str, df_embedding: dict[(str, str), np.array]) -> list[(float, (str, str))]:\n",
    "    query_embedding = get_text_embedding(query)\n",
    "\n",
    "    document_similarities = sorted([\n",
    "        (calculate_vector_similarity(query_embedding, doc_embedding), doc_index) for doc_index, doc_embedding in df_embedding.items()\n",
    "    ], reverse=True)\n",
    "\n",
    "    return document_similarities\n",
    "\n",
    "\n",
    "get_docs_with_similarity(\"In which element, skaters are not holding from each other while skating?\", csv_embeddings)[:3]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create_prompt\n",
    "\n",
    "Prompt is initialized in this function. It takes in the question, embeddings of context and the CSV-file as a dataframe and returns a prepared string (JSON-array). Script counts similarities from the documents for the questions until the used tokens reach at least 500. Script adds system message to the message and includes chosen sections of the source data to the variable *joined_content* and creates the user prompt. Script returns the prepared prompt for the OpenAI Service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "separator_len = len(encoding.encode(\"\\n\" ''))\n",
    "\n",
    "def create_prompt(question: str, context_embeddings: dict, df: pd.DataFrame) -> str:\n",
    "    relevant_document_selections = get_docs_with_similarity(question, context_embeddings)\n",
    "\n",
    "    chosen_sections = []\n",
    "    chosen_sections_len = 0\n",
    "    chosen_sections_indexes = []\n",
    "\n",
    "    for _, section_index in relevant_document_selections:\n",
    "        document_section = df.loc[section_index]\n",
    "\n",
    "        chosen_sections_len += document_section.tokens + separator_len\n",
    "        if chosen_sections_len >= 500:\n",
    "            break\n",
    "\n",
    "        chosen_sections.append(\"\\n- \" + document_section.summarized.replace(\"\\n\", \" \"))\n",
    "        chosen_sections_indexes.append(str(section_index))\n",
    "\n",
    "    systemMsg = \"\"\"Answer the question truthfully and to the best quality you can using the provided context. If there is not an answer in the data, say I do not have the data.\"\"\"\n",
    "    joined_content = \"\".join(chosen_sections) + \"\\n\\n Q: \" + question + \"\\n A:\"\n",
    "    message_text = [\n",
    "    {\n",
    "        \"role\":\"system\",\n",
    "        \"content\": systemMsg \n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": joined_content\n",
    "    }\n",
    "    ]\n",
    "\n",
    "    return  message_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get_answer\n",
    "\n",
    "Script takes in query, source CSV as dataframe and dictionary of document embeddings. It runs the prompt towards the OpenAI Service and returns the response aas a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer(\n",
    "    query: str,\n",
    "    df: pd.DataFrame,\n",
    "    document_embeddings: dict[(str, str), np.array]\n",
    ") -> str:\n",
    "    prompt = create_prompt(\n",
    "        query,\n",
    "        document_embeddings,\n",
    "        df\n",
    "    )\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        messages=prompt,\n",
    "        temperature = 0,\n",
    "        max_tokens = 1000,\n",
    "        model = gpt_model_name\n",
    "        )\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"If you should make an interesting program with five different elements, which one would you choose and why?\"\n",
    "response = get_answer(query, df, csv_embeddings)\n",
    "print(f\"\\nQ: {query}\\nA: {response.choices[0].message.content}\")\n",
    "print(\"Cost: \", response.usage.total_tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
